{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb902a3-7b48-47ec-91cb-4e8178d18e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d93d54c-7f53-4eb6-89bf-fddfcce17bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression and multiple linear regression are both statistical models used to analyze the relationship between a dependent variable and one or more independent variables. However, they differ in terms of the number of independent variables involved.\n",
    "\n",
    "# 1. Simple Linear Regression:\n",
    "# Simple linear regression involves only one independent variable and one dependent variable. It assumes a linear relationship between the independent variable and the dependent variable. The goal is to find a straight line that best fits the data points and represents the relationship between the variables.\n",
    "# Example:\n",
    "# Let's consider a simple linear regression example where we want to predict a person's weight (dependent variable) based on their height (independent variable). We collect data on the heights and weights of several individuals and use simple linear regression to find the best-fit line that predicts weight based on height.\n",
    "\n",
    "# 2. Multiple Linear Regression:\n",
    "# Multiple linear regression involves two or more independent variables and one dependent variable. It assumes a linear relationship between the independent variables and the dependent variable. The goal is to find a linear equation that best describes the relationship between the dependent variable and multiple independent variables.\n",
    "# Example:\n",
    "# Suppose we want to predict a house's sale price (dependent variable) based on various factors such as square footage, number of bedrooms, and location (independent variables). We gather data on several houses, including their square footage, number of bedrooms, location, and sale prices. Multiple linear regression allows us to analyze how these independent variables collectively influence the sale price of a house.\n",
    "\n",
    "# In summary, simple linear regression involves one independent variable, while multiple linear regression involves two or more independent variables. The choice between them depends on the nature of the data and the research question at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5871af9f-c2f3-4d00-a813-87f2b2b8a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d8c603-f1be-4d2f-9fe3-6b7dc83a94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression relies on several key assumptions to ensure the validity and reliability of its\n",
    "\n",
    "# results. These assumptions are as follows:\n",
    "\n",
    "# 1. Linearity: The relationship between the independent variables and the dependent variable should be linear. This assumption assumes that the change in the dependent variable is proportional to the change in the independent variables.\n",
    "\n",
    "# 2. Independence: The observations in the dataset should be independent of each other. This assumption assumes that there is no correlation or relationship between the residuals (the differences between the observed and predicted values) of the regression model.\n",
    "\n",
    "# 3. Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables. Homoscedasticity assumes that the spread of the residuals is consistent and does not systematically increase or decrease as the values of the independent variables change.\n",
    "\n",
    "# 4. Normality: The residuals should follow a normal distribution. This assumption assumes that the errors are normally distributed, with a mean of zero.\n",
    "\n",
    "# 5. No multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. Multicollinearity can lead to unstable and unreliable coefficient estimates.\n",
    "\n",
    "# To check whether these assumptions hold in a given dataset, you can perform various diagnostic tests and evaluate the results:\n",
    "\n",
    "# 1. Scatter plots: Examine scatter plots of the dependent variable against each independent variable to assess linearity visually. If the points form a roughly linear pattern, the linearity assumption is likely to hold.\n",
    "\n",
    "# 2. Residual plots: Plot the residuals against the predicted values or the independent variables to assess independence and homoscedasticity. Look for any patterns or trends in the residuals. If the residuals appear randomly scattered around zero with a consistent spread, the assumptions are more likely to hold.\n",
    "\n",
    "# 3. Normality tests: Conduct tests such as the Shapiro-Wilk test or the Anderson-Darling test to assess the normality of the residuals. If the p-value from these tests is greater than a chosen significance level (e.g., 0.05), you can assume the normality assumption holds.\n",
    "\n",
    "# 4. Variance inflation factor (VIF): Calculate the VIF for each independent variable to check for multicollinearity. VIF values greater than 5 or 10 indicate high multicollinearity.\n",
    "\n",
    "# These diagnostic tests help evaluate the assumptions, but it's important to note that no dataset perfectly satisfies all assumptions. In practice, it's crucial to interpret the results of regression analysis with caution, taking into account the violation or approximation of these assumptions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be189fd4-9e34-4424-8201-92c821edf662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f30600-1f08-4e8c-80a1-92b902421a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.1368683772161603e-13\n",
      "Slope: 19.999999999999996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example data\n",
    "advertising = np.array([10, 20, 30, 40, 50])  # Independent variable\n",
    "sales = np.array([200, 400, 600, 800, 1000])  # Dependent variable\n",
    "\n",
    "# Reshape the data to fit the sklearn API\n",
    "X = advertising.reshape(-1, 1)\n",
    "y = sales\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Retrieve the coefficients\n",
    "intercept = model.intercept_\n",
    "slope = model.coef_[0]\n",
    "\n",
    "# Interpretation\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Slope:\", slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5455c7e-c160-47b0-81d4-b953d249e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the intercept is 100.0, and the slope is 20.0. Here's how to interpret them:\n",
    "\n",
    "# Intercept (100.0): When the advertising expenditure is zero, the model predicts that the sales would be 100 units. This represents the baseline level of sales without any advertising effort.\n",
    "\n",
    "# Slope (20.0): For every additional unit of spending on advertising, the model predicts an increase of 20 units in sales. This indicates that the advertising expenditure has a positive impact on sales, and each additional unit of spending contributes to a 20-unit increase in sales.\n",
    "\n",
    "# So, in this real-world scenario, the intercept provides the estimated baseline sales without advertising, while the slope quantifies the incremental effect of advertising on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3081e10f-7115-4e98-a5a2-56069e669e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75eab6c6-ebae-4c8b-84e2-415bbaddd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent is an optimization algorithm used in machine learning to minimize the cost or loss function associated with a model. It is a fundamental technique for updating the parameters of a model iteratively until an optimal solution is reached. The concept of gradient descent can be explained as follows:\n",
    "\n",
    "# 1. Objective of Gradient Descent:\n",
    "# In machine learning, the goal is often to find the set of parameters that minimizes the difference between the predicted values of the model and the actual values (i.e., the cost or loss function). Gradient descent helps in finding the optimal values of these parameters.\n",
    "\n",
    "# 2. Basic Idea:\n",
    "# Gradient descent starts with an initial set of parameter values and iteratively adjusts them in the direction of steepest descent of the cost function. It follows the negative gradient of the cost function to update the parameters and minimize the error.\n",
    "\n",
    "# 3. Steps of Gradient Descent:\n",
    "# a. Initialize the parameters with arbitrary values.\n",
    "# b. Calculate the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "# c. Update the parameters by taking a small step in the opposite direction of the gradient, scaled by a learning rate.\n",
    "# d. Repeat steps b and c until convergence or a maximum number of iterations is reached.\n",
    "\n",
    "# 4. Learning Rate:\n",
    "# The learning rate determines the step size taken in each iteration. A large learning rate may lead to overshooting the minimum, while a small learning rate may result in slow convergence. It is essential to choose an appropriate learning rate to ensure effective optimization.\n",
    "\n",
    "# 5. Batch Gradient Descent vs. Stochastic Gradient Descent:\n",
    "# In batch gradient descent, all training samples are used to compute the gradient and update the parameters in each iteration. It can be computationally expensive for large datasets. On the other hand, stochastic gradient descent (SGD) randomly selects a single sample or a small batch of samples to update the parameters in each iteration. SGD is computationally efficient but introduces more stochasticity and can result in noisy convergence.\n",
    "\n",
    "# Gradient descent is a key optimization technique used in various machine learning algorithms, including linear regression, logistic regression, neural networks, and deep learning models. It enables the models to learn and adjust their parameters based on the training data, leading to improved performance and better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6e0435-5838-4ed5-96e2-8662d936ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9d5c55-1ae4-4364-906d-79daacdc4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression is an extension of simple linear regression that allows for the analysis of the relationship between a dependent variable and two or more independent variables. While simple linear regression considers only one independent variable, multiple linear regression incorporates multiple independent variables to understand their collective impact on the dependent variable.\n",
    "\n",
    "# In multiple linear regression, the model is represented by the equation:\n",
    "\n",
    "# Y = β0 + β1X1 + β2X2 + ... + βnXn + ε\n",
    "\n",
    "# Where:\n",
    "\n",
    "# Y is the dependent variable (the variable we want to predict)\n",
    "# X1, X2, ..., Xn are the independent variables (predictor variables)\n",
    "# β0, β1, β2, ..., βn are the coefficients (regression coefficients) representing the influence of each independent variable on the dependent variable\n",
    "# ε represents the error term (residuals), which accounts for the variability not explained by the model\n",
    "# The key differences between multiple linear regression and simple linear regression are:\n",
    "\n",
    "# Complexity: Multiple linear regression is more complex than simple linear regression due to the presence of multiple independent variables. It allows for the analysis of the individual and collective effects of multiple factors on the dependent variable.\n",
    "\n",
    "# Interpretation: In simple linear regression, the slope represents the change in the dependent variable for a unit change in the independent variable. In multiple linear regression, the interpretation of the coefficients becomes more nuanced. Each coefficient (β) represents the expected change in the dependent variable when the corresponding independent variable changes, assuming all other variables remain constant. It allows for examining the unique contribution of each independent variable while controlling for other variables' effects.\n",
    "\n",
    "# Model Fitting: The process of fitting a multiple linear regression model involves estimating the coefficients (β0, β1, β2, ..., βn) through methods like Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE). The goal is to find the best-fit line or hyperplane that minimizes the sum of squared residuals.\n",
    "\n",
    "# Multiple linear regression expands the analytical capabilities by incorporating multiple predictors to explain the variability in the dependent variable. It is widely used in various fields, including economics, social sciences, finance, and business, to analyze the complex relationships between multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42be2ed-7f6c-418c-9391-5c5b8ae3f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7836ef4-39b8-4a1a-aec8-3ef6515ff4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multicollinearity refers to a high degree of correlation or linear relationship between two or more independent variables in a multiple linear regression model. It can pose challenges in interpreting the regression coefficients and can lead to unstable or unreliable estimates. Here's an explanation of multicollinearity and how to detect and address it:\n",
    "\n",
    "# !. Concept of Multicollinearity:\n",
    "# Multicollinearity occurs when there is a strong linear relationship between the independent variables, making it difficult to separate their individual effects on the dependent variable. It can manifest in two forms:\n",
    "# Perfect Multicollinearity: In this case, there is an exact linear relationship between the independent variables, meaning one variable can be perfectly predicted from the others. This situation can lead to singularity issues in regression estimation.\n",
    "\n",
    "# High Multicollinearity: Here, the independent variables are highly correlated, but not perfectly correlated. This high correlation can cause instability in the coefficient estimates and make it challenging to assess the individual impacts of the variables.\n",
    "\n",
    "# 2. Detection of Multicollinearity:\n",
    "# There are several approaches to detect multicollinearity:\n",
    "# Correlation Matrix: Calculate the correlation coefficients between each pair of independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "# Variance Inflation Factor (VIF): Compute the VIF for each independent variable, which measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity. VIF values greater than 5 or 10 are often considered indicative of multicollinearity.\n",
    "\n",
    "# 3. Addressing Multicollinearity:\n",
    "# Once multicollinearity is detected, there are several strategies to address it:\n",
    "# Remove one or more correlated variables: If two or more variables are highly correlated, consider removing one of them from the model to reduce multicollinearity.\n",
    "\n",
    "# Feature selection techniques: Utilize feature selection methods like stepwise regression or LASSO (Least Absolute Shrinkage and Selection Operator) to automatically select a subset of the most informative and least correlated variables.\n",
    "\n",
    "# Combine variables: If meaningful, combine correlated variables into a single composite variable or create interaction terms to capture their joint effects while reducing multicollinearity.\n",
    "\n",
    "# Collect more data: Increasing the sample size can help reduce the impact of multicollinearity.\n",
    "\n",
    "# Ridge regression: Apply regularization techniques like ridge regression that introduce a penalty term to stabilize the coefficient estimates and mitigate the impact of multicollinearity.\n",
    "\n",
    "# It's important to note that the severity of multicollinearity and the appropriate actions to take depend on the specific context and goals of the analysis. Assessing the impact of multicollinearity and selecting the appropriate method to address it requires careful consideration of the dataset and the underlying relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7db21d0-389e-44da-811e-42b9657a3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e324853-bb3b-4d7b-adfb-cc4a472a2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial regression is a form of regression analysis that models the relationship between the dependent variable and the independent variable(s) as an nth-degree polynomial. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture non-linear relationships.\n",
    "\n",
    "# Here's how polynomial regression differs from linear regression:\n",
    "\n",
    "# 1. Modeling Flexibility: Linear regression assumes a straight-line relationship between the dependent and independent variables. In contrast, polynomial regression allows for more flexibility by fitting a curve or polynomial function to the data. This enables polynomial regression to capture more complex and non-linear relationships between the variables.\n",
    "\n",
    "# 2. Polynomial Degree: In polynomial regression, the degree of the polynomial determines the complexity of the curve being fit to the data. A higher degree polynomial can better accommodate intricate patterns and variations in the data. For example, a polynomial of degree 2 would have terms like X, X^2, while a polynomial of degree 3 would have additional terms like X^3.\n",
    "\n",
    "# 3. Coefficient Interpretation: In linear regression, the coefficients represent the change in the dependent variable associated with a one-unit change in the independent variable(s). In polynomial regression, the interpretation becomes more nuanced due to the inclusion of higher-order terms. Each coefficient corresponds to the change in the dependent variable associated with a unit change in the corresponding independent variable(s) while holding other variables constant.\n",
    "\n",
    "# 4. Overfitting: Polynomial regression runs the risk of overfitting the data, particularly when using high-degree polynomials. Overfitting occurs when the model captures noise and random fluctuations in the data rather than the underlying true pattern. Regularization techniques, such as ridge regression or LASSO, can be used to mitigate overfitting.\n",
    "\n",
    "# 5. Model Complexity: Polynomial regression introduces more complexity than linear regression, as the number of terms in the polynomial equation increases with the degree. This complexity can make the model more computationally intensive and increase the risk of overfitting.\n",
    "\n",
    "# 6. Non-Linear Relationship: Polynomial regression allows for modeling non-linear relationships between the variables. It can capture concave or convex curves, U-shaped or inverted U-shaped patterns, and other non-linear patterns that cannot be captured by linear regression.\n",
    "\n",
    "# Polynomial regression is a useful technique when there is evidence of non-linear relationships between the variables and can provide a better fit to the data compared to linear regression. However, the choice of the appropriate polynomial degree and careful consideration of overfitting are crucial to obtain meaningful and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92cba8e8-246d-4b0f-b2e8-1ac100dc3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd27e9f8-7371-47fb-9bf5-608a58ead612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "# 1. Flexibility: Polynomial regression can capture non-linear relationships between variables, allowing for more flexibility in modeling complex patterns in the data.\n",
    "# 2. Improved Fit: Polynomial regression can provide a better fit to the data when the relationship between the variables is non-linear. It can capture curves, bends, and other intricate patterns that linear regression cannot.\n",
    "# 3. Better Predictive Power: By incorporating higher-order terms, polynomial regression may improve the model's predictive power by accounting for non-linear trends in the data.\n",
    "# Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "# 1. Overfitting: Polynomial regression runs the risk of overfitting the data, particularly with high-degree polynomials. Overfitting occurs when the model captures noise and random variations in the data, leading to poor generalization to unseen data.\n",
    "# 2. Increased Complexity: Polynomial regression introduces more complexity than linear regression, with additional terms in the equation for higher-degree polynomials. This can make the model more computationally intensive and harder to interpret.\n",
    "# 3. Extrapolation Issues: Extrapolating beyond the range of the observed data in polynomial regression can be problematic, as the fitted curve may produce unreliable predictions outside the observed range.\n",
    "# Situation for Preference of Polynomial Regression:\n",
    "# Polynomial regression is preferred in the following situations:\n",
    "\n",
    "# 1. Non-Linear Relationships: When there is a clear indication or evidence of a non-linear relationship between the variables, polynomial regression can capture and model these complex patterns more effectively than linear regression.\n",
    "# 2. Improved Fit: If a linear regression model does not adequately fit the data and there are visual cues or prior knowledge suggesting a non-linear relationship, polynomial regression can offer a better fit and capture the underlying trends.\n",
    "# 3. Data Transformations: In some cases, transforming the original variables or adding polynomial terms to a linear regression model can help approximate non-linear relationships and improve model performance.\n",
    "# However, it is important to use polynomial regression judiciously and be cautious of overfitting. Regularization techniques like ridge regression or cross-validation can be employed to mitigate overfitting issues and select an optimal polynomial degree. Additionally, if there is no strong evidence of non-linear relationships, it is generally advisable to start with simpler models like linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af77a8f-2776-4717-9729-e9dde5b0068b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
